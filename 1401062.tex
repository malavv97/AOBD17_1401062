%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%2345678901234567890123456789012345678901234567890123456789012345678901234567890
%        1         2         3         4         5         6         7         8

\documentclass[letterpaper, 10 pt, conference]{ieeeconf}  % Comment this line out
                                                          % if you need a4paper
%\documentclass[a4paper, 10pt, conference]{ieeeconf}      % Use this line for a4
                                                          % paper

\IEEEoverridecommandlockouts                              % This command is only
                                                          % needed if you want to
                                                          % use the \thanks command
\overrideIEEEmargins
% See the \addtolength command later in the file to balance the column lengths
% on the last page of the document



% The following packages can be found on http:\\www.ctan.org
%\usepackage{graphics} % for pdf, bitmapped graphics files
%\usepackage{epsfig} % for postscript graphics files
%\usepackage{mathptmx} % assumes new font selection scheme installed
%\usepackage{times} % assumes new font selection scheme installed
%\usepackage{amsmath} % assumes amsmath package installed
%\usepackage{amssymb}  % assumes amsmath package installed

\title{\LARGE \bf \centering
Project Report: Robust Principle Component Analysis
}
%\author{ \parbox{3 in}{\centering Huibert Kwakernaak*
%         \thanks{*Use the $\backslash$thanks command to put information here}\\
%         Faculty of Electrical Engineering, Mathematics and Computer Science\\
%         University of Twente\\
%         7500 AE Enschede, The Netherlands\\
%         {\tt\small h.kwakernaak@autsubmit.com}}
%         \hspace*{ 0.5 in}
%         \parbox{3 in}{ \centering Pradeep Misra**
%         \thanks{**The footnote marks may be inserted manually}\\
%        Department of Electrical Engineering \\
%         Wright State University\\
%         Dayton, OH 45435, USA\\
%         {\tt\small pmisra@cs.wright.edu}}
%}

\begin{document}

\maketitle
\thispagestyle{empty}
\pagestyle{empty}

\begin{abstract}

In this paper, we talk about a data matrix which is superposition of a low rank component and a sparse component. The question here is can we recover both low rank and sparse components individually from the data matrix? Under some suitable assumptions it is possible to recover both these components exactly by a very convenient convex program called Principal Component Pursuit; among all feasible decompositions, simply minimize a weighted combination of the nuclear norm and of the l1 norm. This paper also allows us to recover the principle components if some of its entries are arbitrarily corrupted. This also holds true if some of its entries are missing.  
\end{abstract}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{INTRODUCTION}

Suppose, that we are given a matrix M which can be decomposed in two parts such that
$$
M = L_{0} + S_{0} \eqno{(1)}
$$

where $L_{0}$ is a low rank matrix and $S_{0}$ is a sparse matrix. We do not know the dimensions of either of them. A solution to this problem might prove to be very important in today's data intensive process of scientific discovery. In current application domains of this problem, data is usually in millions or billions of dimensions. So we also need to reduce the dimensionality and bring the data in lesser dimensions. So we convert our large data matrix into low rank matrix $L_{0}$. Classical Principle Component Analysis seeks the best rank-k estimate of $L_{0}$ by solving
$$
minimze \quad ||M - L||
$$
$$
subject \: to \quad rank(L) \leq k
$$


PCA is the most widely used statistical tool for data analysis and dimensionality reduction. Many approaches have been proposed but none of them yield a polynomial time algorithm with strong performance guarantees under broad conditions.The problem we study here can be considered as an idealized version of Robust PCA, in which out aim is to recover a low rank matrix $L_{0}$ from highly corrupted measurements M = $L_{0}$ + $S_{0}$. The entries in $S_{0}$ can have arbitarily high magnitude but is sparse. Seperation makes sense when a matrix M is either sparse or low rank and not both. Suppose M is a mtrix $e_{1}e_{1}^{*}$. Then M is both sparse and low rank. So to make problem meaningful we need to make sure matrix $L_{0}$ is low rank and not sparse. Using the general notion of incoherence introduced in Candès and Recht [2009] for the matrix completion problem; this is an assumption concerning the singular vectors of the low-rank component. The SVD of $L_{0}$ can be written as where $L_{0}$ is a $n_{1} \times n_{2}$ matrix

$$
L_{0} = U\Sigma V^{*}
$$

where $\Sigma$ has positive singular values and U and V are matrices of left and right singular vectors. Thus the incoherence condition with parameter $\mu$ states that 

$$
max ||U^{*}e_{i}||^{2} \leq \frac{\mu r}{n_{1}}, max ||V^{*}e_{i}||^{2} \leq \frac{\mu r}{n_{2}} \eqno{(2)}
$$

$$
|| UV^{*}||_{\infty} \leq \sqrt[]{\frac{\mu r}{n_{1}n_{2}}} \eqno{(3)}
$$

Here, $||M||_{\infty}$ = $max_{i,j} |M_{ij}|$, that is, is the $l_{\infty}$ norm of M seen as a long vector. Note that since the orthogonal projection $P_{U}$ onto the column space of U is given by $P_{U}$ = $UU^{*}$, (2) is equivalent to $max_{i} ||P_{U} e_{i}||^{2} \leq \frac{\mu r}{n_{1}}$, and similarly for $P_{V}$.
Another issue arises if the sparse matrix is low rank. To avoid such situations we assume that the sparsity pattern of the sparse component is selected uniformly at random.

If we have a matrix M of dimensions $n_{1} \times n_{2}$ then under some weak assumptions the Principal Component Pursuit estimate solving 

$$
minimze \quad ||L||_{*} + \lambda||S||_{1}
$$
$$
subject \: to \quad L + S = M
$$

exactly recovers matrix $L_{0}$ and $S_{0}$. This is valid even if entries in low rank matrices increases linearly in the dimensions and the errors in sparse matrix are upto a constant fraction of all entries. Algorithmically, we will see that this problem can be solved by efficient and scalable algorithms, at a cost not so much higher than the classical PCA.

\section{Applications}

There are many important applications in which the data can be modeled as a low rank and sparse matrix. 

\subsection{Video Surveillance}

Suppose we are given a surveillance frames of a video, we need to identify activities that stand out from background. If we stack video as a matrix, then the low rank matrix corresponds to the stationary background and sparse matrix captures moving objects from the video. 

\subsection{Face Recognition}

 It is well known that images of a convex, Lambertian surface un-
der varying illuminations span a low-dimensional subspace. This fact has been a main reason why low-dimensional models are mostly effective
for imagery data. In particular, images of a human’s face can be well-approximated by a low-dimensional subspace. Being able to correctly retrieve this subspace is crucial in many applications such as face recognition and alignment. However, realistic face images often suffer from self-shadowing, specularities, or saturations in brightness, which make this a difficult task and subsequently compromise the recognition performance.

\subsection{Ranking and Collaborative Filtering}
The problem of anticipating user tastes is gaining increasing importance in online commerce and advertisement. Companies now routinely collect user rankings for various products, for example, movies, books, games, or web tools, among which the Netflix Prize for movie ranking is the best known [Netflix, Inc.]. The problem is to use incomplete rankings provided by the users on some of the products to predict the preference of any given user on any of the products. This problem is typically cast as a low-rank matrix completion problem. However, as the data collection process often lacks control or is sometimes even ad hoc—a small portion of the available rankings could be noisy and even tampered with. The problem is more challenging since we need to simultaneously complete the matrix and correct the errors.

Similar problems also arise in many other applications such as Ranking and collaborative Filtering, Latent Semantic Indexing, graphical model learning, linear system identification, and coherence decomposition in optical systems
\section{Main Result}

\subsection{Theorem 1.1}

Suppose $L_{0}$ is n x n, obeys equations (2) and (3). Fix any n x n matrix $\Sigma$ of signs. Suppose that the support set $\Omega$ of $S_{0}$ is uniformly distributed among all sets of cardinality m, and that sgn($[S_{0}]_{ij}$) = $\Sigma_{ij}$ for all (i, j) $\in$ $\Omega$. Then, there is a numerical constant c such that with probability at least 1 - $cn^{-10}$ (over the choice of support of $S_{0}$), Principal Component Pursuit (1) with $\lambda$ = 1/ $\sqrt[]{n}$ is exact, that is, $\hat{L}$ = $L_{0}$ and $\hat{S}$ = $S_{0}$, provided that 

$$
rank(L_{0}) \leq \rho_{r} n \mu ^{-1} (logn)^{-2} \: and \: m \leq \rho_{s} n^{2} \eqno{(4)}
$$

In this equation, $\rho_{r}$ and $\rho_{s}$ are positive numerical constants. In the general rectangular case, where $L_{0}$ is $n_{1} x n_{2}$ , PCP with $\lambda$ = $\frac{1}{\sqrt[]{n_{(1)}}}$ succeeds with probability at least $1 - cn^{-10}_{(1)}$ , provided that rank($L_{0}$) $\leq \rho_{r} n_{(2)} \mu^{-1} (logn_{(1)})^{-2}$ and $m \leq \rho_{s} n_{1} n_{2}$.
\\
Matrix $L_{0}$ whose principal components are spreaded can be recovered with probability almost onefrom arbitary and completely unknown corruption patterns. It also works for higher ranks like n/$log(n)^{2}$ when $\mu$ is not large. 
Minimizing 
$$
||L||_{*} + \frac{1}{\sqrt[]{n_{(1)}}} ||S||_{1}
$$

where,
$$
n_{(1)} = max(n_{1}, n_{2})
$$

under the assumption of theorem, this always gives correct answer. Here we chose $\lambda$ = $\frac{1}{\sqrt[]{n_{(1)}}}$ but it is not clear why that has happened. It has been due to mathematical analysis why we are taking that value. 

\section{CONCLUSION}

From the above results we can say that if conditions in equation (2) and (3) are satisfied then we can get a low rank and sparse matrix from matrix M. The main phenomena is to select a uniformly random sparse matrix. So this approach can be used to decompose a given matrix in two components i.e. low rank and sparse matrix. 

\addtolength{\textheight}{-12cm}   % This command serves to balance the column lengths
                                  % on the last page of the document manually. It shortens
                                  % the textheight of the last page by a suitable amount.
                                  % This command does not take effect until the next page
                                  % so it should come on the page before the last. Make
                                  % sure that you do not shorten the textheight too much.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{thebibliography}{99}

\bibitem{c1} Candès, E. J., Li, X., Ma, Y., and Wright, J. 2011. Robust principal component analysis? J. ACM 58, 3, Article 11 (May 2011), 37 pages.
\bibitem{c2} Yi Ma, Compressive Principal Component Pursuit, 2012
\bibitem{c3} C HANDRASEKARAN , V., S ANGHAVI , S., P ARRILO , P., AND W ILLSKY , A. 2009. Rank-sparsity incoherence for matrix
decomposition. Siam J. Optim.






\end{thebibliography}




\end{document}
